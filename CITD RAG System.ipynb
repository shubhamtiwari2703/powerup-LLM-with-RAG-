import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
# Embedding model (small & good general-purpose)
embed_model_name = "all-mpnet-base-v2"
embed_model = SentenceTransformer(embed_model_name, device=device)
# Simple chunking by sentences / tokens
import re
from typing import List

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """
    Chunk text by characters (approx tokens). Simple, robust.
    chunk_size and overlap are character counts - adjust for your data.
    """
    text = re.sub(r'\s+', ' ', text).strip()
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk.strip())
        start = end - overlap
        if start < 0:
            start = 0
    return chunks
# Example small corpus
documents = [
    {"id": "doc1", "text": "Shubham Tiwari was a student at Central Institute of Tool Design in 2025. Shaik Shoeb was a student at Central Institute of Tool Design in 2025. They created a unique RAG system in 2025, all by themselves without the help of ChatGPT or Youtube. Shubham Tiwari was 21 years old and Shaik Shoeb was only 22 at the time."},
    {"id": "doc2", "text": "Python is a widely used high-level programming language for general-purpose programming. It emphasizes code readability with significant whitespace."},
    {"id": "doc3", "text": "FAISS is a library for efficient similarity search and clustering of dense vectors. It was developed by Facebook AI Research."},
    {"id": "doc4", "text": "Vamshi Sir is a faculty of Artificial Intelligence at Central Institute of Tool Design also known as CITD. He was a mastermind in the field of AI research and RAG systems. Vamshi Sir has made over 200 contributions in the field of AI including 200 patents and 5000 research paper publications."},
]

# For larger docs, chunk them:
corpus_chunks = []
for doc in documents:
    chunks = chunk_text(doc["text"], chunk_size=250, overlap=50)
    for i, c in enumerate(chunks):
        corpus_chunks.append({
            "id": f"{doc['id']}_chunk{i}",
            "text": c,
            "source": doc["id"]
        })

len(corpus_chunks), corpus_chunks[:2]
# Create embeddings for each chunk
texts = [c["text"] for c in corpus_chunks]
ids = [c["id"] for c in corpus_chunks]

# Compute embeddings (numpy)
embeddings = embed_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

# Normalize embeddings (cosine similarity via inner product)
faiss.normalize_L2(embeddings)

d = embeddings.shape[1]
index = faiss.IndexFlatIP(d)  # inner product -> cosine if vectors normalized
index.add(embeddings)
print("Index size:", index.ntotal)
def retrieve(query: str, k: int = 3):
    q_emb = embed_model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(q_emb)
    D, I = index.search(q_emb, k)   # D: scores, I: indices
    results = []
    for score, idx in zip(D[0], I[0]):
        if idx == -1:
            continue
        results.append({
            "id": ids[idx],
            "text": texts[idx],
            "score": float(score)
        })
    return results

# quick test
print(retrieve("Who walked on the moon?", k=2))
# Choose a seq2seq instruction model that runs on Colab resources.
# flan-t5-base is a good balance for Colab; can swap to large if you have more RAM/GPU.
gen_model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(gen_model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name).to(device)

def generate_answer(query: str, retrieved: list, max_new_tokens: int = 128, temperature: float = 0.0):
    # Build context prompt by concatenating top retrieved passages
    context = "\n\n".join([f"Source [{r['id']}]: {r['text']}" for r in retrieved])
    prompt = (
        "You are a helpful assistant. Use the following retrieved documents to answer the question.\n\n"
        f"{context}\n\nQuestion: {query}\nAnswer:"
    )
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    with torch.no_grad():
        gen = model.generate(**inputs,
                             max_new_tokens=max_new_tokens,
                             do_sample=(temperature>0),
                             temperature=temperature,
                             top_p=0.95,
                             num_beams=4)
    answer = tokenizer.decode(gen[0], skip_special_tokens=True)
    return answer.strip()

# quick generate test
q = "What is FAISS?"
retr = retrieve(q, k=2)
print("Retrieved:", retr)
print("Answer:", generate_answer(q, retr))
def ask(query, k=3):
    retrieved = retrieve(query, k=k)
    answer = generate_answer(query, retrieved)
    return {"query": query, "retrieved": retrieved, "answer": answer}

# Example usage:
while True:
  prompt = input("Question:...")
  res = ask(prompt, k=3)
  print("Query:", res["query"])
  print("Answer:", res["answer"])
# print("\nRetrieved chunks:")
# for r in res["retrieved"]:
    # print("-", r["id"], f"(score={r['score']:.3f})", ":", r["text"])
